{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c6f1e5-349b-41a6-aefa-3007b5a8817c",
   "metadata": {},
   "source": [
    "# Recommending songs by embeddings\n",
    "\n",
    "**NOTE:** This notebook is based on the tutorial in Chapter 2 of *[Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)* by [Jay Alammar](https://www.linkedin.com/in/jalammar/) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
    "\n",
    "The idea here is that we have a bunch of song playlists like this...\n",
    "\n",
    "- Rossana * Billy Jean * Let's go crazy * etc.\n",
    "- Fack to black * Between the lines * One * etc.\n",
    "\n",
    "...and the word embedding model will cluster songs that appear next to each other in a bunch of playlists. We can then use those similarities to generate new playlists based on individual songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0657da-b051-4db2-aad6-19b02f99fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim # we use gensim to download a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d5a1b-b3a7-45b4-8160-35d66c6658f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules we'll need\n",
    "import urllib.request\n",
    "from gensim.models import word2vec # We will train a word2vec model with playlist data\n",
    "import pandas as pd # we'll use pandas to format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880ecf7-ddf1-448c-8606-c934bb4fcfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in a tab-delimited file that contains song id numbers\n",
    "## paired with song names and artists.\n",
    "# id_to_title = pd.read_csv(\"song_hash.txt\", sep=\"\\t\", \n",
    "#                           header=None, \n",
    "#                           names=[\"id\", \"title\", \"artist\"])\n",
    "id_to_title = pd.read_csv(\"https://raw.githubusercontent.com/StatQuest/embeddings_for_recommendations/main/song_hash.txt\", \n",
    "                          sep=\"\\t\", \n",
    "                          header=None, \n",
    "                          names=[\"id\", \"title\", \"artist\"])\n",
    "id_to_title.head() # print out the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5246193-5a53-45ca-bb0a-8a194a23c5eb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b4cbe-3c25-46d9-b6fd-382ba74cea40",
   "metadata": {},
   "source": [
    "# Import the playlist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374260cc-e31e-4299-96a5-b27e39af4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: The data files were originally created by Shuo Chen (shuochen@cs.cornell.edu) \n",
    "##       in the Dept. of Computer Science, Cornell University.\n",
    "## I downloaded them from here: https://www.cs.cornell.edu/~shuochen/lme/data_page.html\n",
    "##\n",
    "## open() opens the file...\n",
    "## read() reads it in...\n",
    "## split('\\n') makes it legible\n",
    "## [2:] skips the first to lines of metadata\n",
    "# data = open(\"train.txt\", \"r\").read().split('\\n')[2:]\n",
    "\n",
    "data = urllib.request.urlopen('https://raw.githubusercontent.com/StatQuest/embeddings_for_recommendations/main/train.txt')\n",
    "data = data.read().decode(\"utf-8\").split('\\n')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb63b0f-819c-4170-b0f2-e40b3e9b2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove playlists with only one song\n",
    "playlists = [s.rstrip().split() for s in data if len(s.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ac37e-039f-476e-b088-e221056c0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
    "print( 'Playlist #2:\\n ', playlists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed1d22-06b5-4b3d-a278-5de59874c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a word embedding model with our playlists\n",
    "## NOTE: By default Word2Vec uses the \"CBOW\" (continuous bag of words) method for \n",
    "##       training. CBOW uses surrounding words to predict a word in the middle.\n",
    "##       For example, if the training set was \"Troll2 is great\", then\n",
    "##       CBOW would use \"Troll2\" and \"great\" to predicet \"is\".\n",
    "## vector_size : dimensionality of the word vectors.\n",
    "## negative : If > 0, negative sampling will be used, \n",
    "##            and specifies how many “noise words” should be drawn (usually between 5-20).\n",
    "## min_count : Ignores all words with total frequency lower than this.\n",
    "## workers : Use these many worker threads to train the model\n",
    "## NOTE: The value I selected for the arguments allowed for relatively fast training and \n",
    "##       worked well enough.\n",
    "model = word2vec.Word2Vec(playlists, vector_size=32, negative=10, min_count=1, workers=4) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8162466-fced-4cfc-836a-e076b44b4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_id = 3822 # Billie Jean - Michael Jackson\n",
    "# song_id = 2172 # Fade To Black - Metallica\n",
    "# song_id = 842 # California Love - 2Pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27817b-dfe0-4f23-af12-3a9a1dc31600",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_title.iloc[song_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d560ee-fd07-41bb-87af-221241b89c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the most similar songs\n",
    "new_playlist = pd.DataFrame(model.wv.most_similar(positive=str(song_id)),\n",
    "                            columns=[\"id\", \"sim\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d783e5-5373-4584-ad8d-bfa3afcdf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5559b-5f06-4127-aff4-bb41a51437ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the song names and artists for the new\n",
    "id_to_title.iloc[new_playlist[\"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c396c0-7c08-4eab-8377-3425af2612f1",
   "metadata": {},
   "source": [
    "# Bam!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
